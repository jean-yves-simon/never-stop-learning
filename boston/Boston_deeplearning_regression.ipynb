{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Deep Learning Case Study: Boston housing price\n",
    "\n",
    "<p style=\"text-align: justify\">In this project tutorial we will discover how to develop and evaluate neural network models\n",
    "using Keras for a regression problem. This project covers the following aspects:</p>\n",
    "\n",
    "\n",
    "\n",
    "<ul>\n",
    "<li>How to load a CSV dataset and make it available to Keras.</li>\n",
    "<li>How to create a neural network model with Keras for a regression problem.</li>\n",
    "<li>How to use scikit-learn with Keras to evaluate models using cross validation.</li>\n",
    "<li>How to perform data preparation in order to improve skill with Keras models.</li>\n",
    "<li>How to tune the network topology of models with Keras.</li>\n",
    "</ul>\n",
    "\n",
    "<p style=\"text-align: justify\">For this project we will investigate the <a href=\"https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data\">Boston House Price</a> dataset. Each record in the database\n",
    "describes a Boston suburb or town. The data was drawn from the Boston Standard Metropolitan Statistical Area (SMSA) in 1970. The attributes are defined as follows (taken from the <a href=\"https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\">UCI Machine Learning Repository</a>):</p>\n",
    "\n",
    "1. CRIM: per capita crime rate by town\n",
    "2. ZN: proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "3. INDUS: proportion of non-retail business acres per town\n",
    "4. CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "5. NOX: nitric oxides concentration (parts per 10 million)\n",
    "6. RM: average number of rooms per dwelling\n",
    "7. AGE: proportion of owner-occupied units built prior to 1940\n",
    "8. DIS: weighted distances to five Boston employment centers\n",
    "9. RAD: index of accessibility to radial highways\n",
    "10. TAX: full-value property-tax rate per &#36;10,000\n",
    "11. PTRATIO: pupil-teacher ratio by town\n",
    "12. B: $1000(Bk - 0:63)^2$ where $Bk$ is the proportion of blacks by town\n",
    "13. LSTAT: % lower status of the population\n",
    "14. MEDV: Median value of owner-occupied homes in &#36;1000s\n",
    "\n",
    "We can see that the input attributes have a mixture of units, which may require normalization of the features.\n",
    "\n",
    "<p style=\"text-align: justify\">Reasonable performance for models evaluated using Mean Squared Error (MSE) are around 20 in squared thousands of dollars (or $4,500 if you take the square root). This is a nice target to aim for with our neural network model.</p>\n",
    "\n",
    "## Develop a Baseline Neural Network Model\n",
    "\n",
    "In this section we will create a baseline neural network model for the regression problem. Let's start off by importing all of the functions and objects we will need for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Load libraries\n",
    "\n",
    "# numpy\n",
    "import numpy as np\n",
    "\n",
    "# pandas\n",
    "import pandas as pd\n",
    "\n",
    "# keras library\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "# scikit functions\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">We can now load our dataset from a file in the local directory. The dataset is in fact not in CSV format on the UCI Machine Learning Repository, the attributes are instead separated by whitespace. We can load this easily using the Pandas library. We can then split the input (<em>X</em>) and output (<em>Y</em>) attributes so that they are easier to model with Keras and scikit-learn.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "dataframe = pd.read_csv(\"housing.csv\", delim_whitespace=True, header=None)\n",
    "dataset = dataframe.values\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:13]\n",
    "Y = dataset[:,13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">We can create Keras models and evaluate them with scikit-learn, by using handy wrapper objects provided by the Keras library. This is desirable, because scikit-learn excels at evaluating models, and will allow us to use powerful data preparation and model evaluation schemes, with very few lines of code. The Keras wrapper class requires a function as an argument. This function, that we must define is responsible for creating the neural network model to be evaluated.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">Below, we define the function to create the baseline model to be evaluated. It is a simple model that has a <b>single fully connected hidden layer</b>, with the same number of neurons as input attributes (13). The network uses good practices such as the rectifier activation function for the hidden layer. No activation function is used for the output layer because it is a regression problem, and we are interested in predicting numerical values directly without transform.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">The efficient <b>ADAM</b> optimization algorithm is used and a <b>mean squared error</b> loss function is optimized. This will be the same metric that we will use to evaluate the performance of the model. It is a desirable metric because by taking the square root of an error value it gives us a result that we can directly understand, in the context of the problem with the units in thousands of dollars.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define base mode\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(13, input_dim=13, activation='relu', kernel_initializer='normal'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">The Keras wrapper object for use in scikit-learn as a regression estimator is called \"KerasRegressor\". We create an instance and pass it to both, the name of the function to create the neural network model, as well as some parameters to pass along to the <em>fit()</em> function of the model later, such as the number of epochs and batch size. Both of these are set to sensible defaults. We also initialize the random number generator with a constant random seed, a process we will repeat for each model evaluated in this tutorial. This is to ensure we compare models consistently and that the results are reproducible.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# evaluate model with standardized dataset\n",
    "estimator = KerasRegressor(build_fn=baseline_model, epochs=50, batch_size=5, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">The final step is to evaluate this baseline model. We will use 10-fold cross validation to\n",
    "evaluate the model.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: -37.47 (21.61) MSE\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(estimator, X, Y, cv=kfold)\n",
    "print(\"Baseline: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lift Performance By Standardizing The Dataset\n",
    "\n",
    "<p style=\"text-align: justify\">An important concern with the Boston house price dataset is that the input attributes all vary\n",
    "in their scales, because they measure different quantities. It is almost always good practice to prepare your data before modeling it using a neural network model. Continuing on from the above baseline model, we can re-evaluate the same model using a standardized version of the input dataset.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">We can use scikit-learn's Pipeline framework to perform the standardization during the model evaluation process, within each fold of the cross validation. This ensures that there is no data leakage from each testset cross validation fold into the training data. The code below creates a scikit-learn Pipeline that first standardizes the dataset, then creates and evaluates the baseline neural network model.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized: -29.61 (27.29) MSE\n"
     ]
    }
   ],
   "source": [
    "#evaluate baseline model with standardized dataset\n",
    "np.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(build_fn=baseline_model, epochs=50, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(pipeline, X, Y, cv=kfold)\n",
    "print(\"Standardized: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">Running the example provides an improved performance over the baseline model without standardized data, dropping the error by 10 thousand squared dollars.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">A further extension of this section would be to similarly apply a rescaling to the output variable such as, <b>normalizing</b> it to the range of 0 to 1, and use a Sigmoid or similar <b>activation function</b> on the <b>output layer</b> to narrow output predictions to the same range.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune The Neural Network Topology\n",
    "\n",
    "<p style=\"text-align: justify\">There are many concerns that can be optimized for a neural network model. Perhaps the point of biggest leverage is the structure of the network itself, including the number of layers and the number of neurons in each layer. In this section we will evaluate two additional network topologies, in an effort to further improve the performance of the model. We will look at both a deeper and a wider network topology.</p>\n",
    "\n",
    "### Evaluate a Deeper Network Topology\n",
    "\n",
    "<p style=\"text-align: justify\">One way to improve the performance of a neural network is to add more layers. This might allow the model to extract and recombine higher order features embedded in the data. In this section, we will evaluate the effect of adding one more hidden layer to the model. This is as easy as defining a new function that will create this deeper model, copied from our baseline model above. We can then insert a new line after the first hidden layer. In this case with about half the number of neurons. Our network topology now looks like:</p>\n",
    "\n",
    "    13 inputs -> [13 -> 6] -> 1 output\n",
    "\n",
    "<p style=\"text-align: justify\">We can evaluate this network topology in the same way as above, whilst also using the standardization of the dataset that above was shown to improve performance.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Larger: -23.33 (27.01) MSE\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "def larger_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(13, input_dim=13, activation='relu', kernel_initializer='normal'))\n",
    "    model.add(Dense(6, activation='relu', kernel_initializer='normal'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# evaluate model with standardized dataset\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(build_fn=larger_model, epochs=50, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(pipeline, X, Y, cv=kfold)\n",
    "print(\"Larger: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">By adding one hidden layer  in the network, the performances almost got unchanged, from 22.8 to 22.1 in terms of mean MSE. The variance even increased, showing that deeper networks, which are also more computationally expensive are not always better in terms of performance and accuracy.</p>\n",
    "\n",
    "### Evaluate a Wider Network Topology\n",
    "\n",
    "<p style=\"text-align: justify\">Another approach to increasing the representational capacity of the model is to create a wider network. In this section we evaluate the effect of keeping a shallow network architecture and nearly doubling the number of neurons in the one hidden layer. Again, all we need to do is define a new function that creates our neural network model. Here, we have increased the number of neurons in the hidden layer compared to the baseline model from 13 to 20. The topology for\n",
    "our wider network can be summarized as follows:</p>\n",
    "\n",
    "    13 inputs -> [20] -> 1 output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wider: -24.64 (25.65) MSE\n"
     ]
    }
   ],
   "source": [
    "# define wider model\n",
    "def wider_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_dim=13, activation='relu', kernel_initializer='normal'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# evaluate model with standardized dataset\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(build_fn=wider_model, epochs=50, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(pipeline, X, Y, cv=kfold)\n",
    "print(\"Wider: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">With the current set of parameters, the deeper network performs better than the wider network. It would have been hard to guess that a deeper network would outperform a wider network on this problem. The results demonstrate the importance of empirical testing when it comes to developing neural network models.</p>\n",
    "\n",
    "## Extensions\n",
    "\n",
    "<p style=\"text-align: justify\">This section lists some ideas for extending the tutorial that you may wish to explore to increase the performance of the neural network:</p>\n",
    "\n",
    "<ul>\n",
    "\n",
    "<li><b>Data Transforms</b>. In the current notebook, standardization has been considered. Normalization of the features (between 0 and 1) could also be experimented to see whether there is some improvement with the different neural networks.</li>\n",
    "<li><b>Hyperparameters grid search</b>. Exploring how the number of epochs, <i>k</i>-folds, or batch size for instance would affect the results.</li>\n",
    "<li><b>Deeper and/or wider networks</b>. See if there is any real improvement in terms of accuracy without sacrificing computational power.</li>\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
